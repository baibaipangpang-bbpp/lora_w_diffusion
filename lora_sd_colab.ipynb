{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtM0VRMI3Nb1"
   },
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
    "FOLDERNAME = \"cs231n/project/\"\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "PROJECT_PATH = f\"/content/drive/My Drive/{FOLDERNAME}\"\n",
    "sys.path.append(PROJECT_PATH)\n",
    "\n",
    "# Change working directory\n",
    "os.chdir(PROJECT_PATH)\n",
    "\n",
    "# Confirm\n",
    "print(\"‚úÖ Current working directory:\", os.getcwd())\n",
    "print(\"üìÅ Contents:\", os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRetkHCR2ke0"
   },
   "source": [
    "# üí° LoRA Training on Stable Diffusion\n",
    "This notebook trains custom LoRA adapters on Stable Diffusion using your own image-caption pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnov-IWl2ke1"
   },
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate torchvision safetensors kornia wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjd5UBjt2ke3"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4907-Xf2ke3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import ImageTextDataset\n",
    "\n",
    "dataset = ImageTextDataset(\"data\")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# sanity check for datasets\n",
    "print(f\"Number of image-caption pairs: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgZYv9Qr-fuV"
   },
   "outputs": [],
   "source": [
    "# preview a couple of images.\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "# This regex will match messages containing \"Glyph\" and \"missing from font\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\"Glyph .* missing from font\\(s\\) DejaVu Sans\\.\")\n",
    "\n",
    "for i in range(min(2, len(dataset))):  # Show up to 3 examples\n",
    "    image, caption = dataset[i]\n",
    "    plt.imshow(image.permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Undo normalization\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(caption)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejszCma45YBl"
   },
   "source": [
    "## Training Logic for Lora Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcqYvlZ_1APe"
   },
   "outputs": [],
   "source": [
    "# Hyper params\n",
    "\n",
    "lora_rank = 8\n",
    "lora_alpha = 8\n",
    "epochs = 100\n",
    "conv_lora = True\n",
    "lr = 1e-4\n",
    "dataset = 'chinese'\n",
    "\n",
    "run_name = f\"dataset_{dataset}_rank_{lora_rank}_alpha_{lora_alpha}_epochs_{epochs}_conv_{int(conv_lora)}_lr_{lr:.0e}\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWHHLkRet1N_"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"20e7c4be307028d246fbff111508b75d9eaab1ee\"\n",
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    project=\"stable-diffusion-calligraphy\",\n",
    "    name=run_name,  # give each experiment a unique name\n",
    "    config={\n",
    "        \"lora_rank\": lora_rank,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"lr\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"augmentations\": False,\n",
    "        \"conv_lora\": conv_lora,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LsuFYFH783B"
   },
   "outputs": [],
   "source": [
    "from lora import LoRALinear, LoRAConv2d\n",
    "from patch_unet import patch_unet_with_lora, conv_filter\n",
    "\n",
    "device = \"cuda\"\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float32).to(device)\n",
    "\n",
    "pipe.vae.requires_grad_(False)\n",
    "pipe.text_encoder.requires_grad_(False)\n",
    "pipe.unet.requires_grad_(False)\n",
    "\n",
    "# ADD LORA\n",
    "patch_unet_with_lora(pipe.unet, r=lora_rank, alpha=lora_alpha, dropout=0.0, conv_filter=None)\n",
    "\n",
    "pipe.unet.to(device)  # Move after patching\n",
    "\n",
    "# unfreeze lora weights\n",
    "for module in pipe.unet.modules():\n",
    "    if isinstance(module, (LoRALinear, LoRAConv2d)):\n",
    "        for p in module.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam([p for p in pipe.unet.parameters() if p.requires_grad], lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJ9tAhsokBbH"
   },
   "outputs": [],
   "source": [
    "# Loads the CLIP model and processor\n",
    "from clip import calculate_clip_score\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "938GzVt2lwta"
   },
   "outputs": [],
   "source": [
    "def generate_and_log_image(pipe, clip_model, clip_processor, device, prompt, epoch):\n",
    "    \"\"\"\n",
    "    Generate an image from a prompt using the pipeline, calculate CLIP score,\n",
    "    and log the result to Weights & Biases.\n",
    "\n",
    "    Args:\n",
    "        pipe: The diffusion pipeline.\n",
    "        clip_model: The CLIP model for scoring.\n",
    "        clip_processor: The processor for CLIP input.\n",
    "        device: torch.device to use.\n",
    "        prompt (str): Text prompt to generate image from.\n",
    "        epoch (int): Current training epoch, for logging.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        image = pipe(prompt, num_inference_steps=30).images[0]\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = calculate_clip_score(image, prompt, clip_model, clip_processor, device)\n",
    "\n",
    "    # Log image, score, and prompt to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"generated_image\": wandb.Image(image, caption=f\"{prompt} | score: {clip_score:.3f}\"),\n",
    "        \"clip_score\": clip_score,\n",
    "        \"generation_prompt\": prompt,\n",
    "    })\n",
    "\n",
    "    return image, clip_score  # optionally return for other use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52d-NbgAA3nY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from augment import augment\n",
    "from patch_unet import save_lora_weights, load_lora_weights\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# training\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, captions) in enumerate(loader):\n",
    "        # Keep images in float32\n",
    "        images = images.to(device)\n",
    "\n",
    "        images = augment(images)\n",
    "\n",
    "        text_input = pipe.tokenizer(captions, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
    "        text_embeds = pipe.text_encoder(**text_input).last_hidden_state.to(device)  # stays float32\n",
    "\n",
    "        with torch.no_grad():\n",
    "            vae_output = pipe.vae.encode(images)\n",
    "        latents = vae_output.latent_dist.sample().to(device) * 0.18215  # float32\n",
    "\n",
    "        noise = torch.randn_like(latents).to(device)  # float32 noise\n",
    "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
    "\n",
    "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "        noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=text_embeds).sample  # float32\n",
    "\n",
    "        loss = nn.MSELoss()(noise_pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch} Step {i} Loss: {loss.item():.4f}\")\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": i,\n",
    "            \"loss\": loss.item(),\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "        })\n",
    "\n",
    "    # Log one of the input images\n",
    "    sample_image = (images[0].cpu().numpy() * 0.5 + 0.5).transpose(1, 2, 0)\n",
    "    wandb.log({\n",
    "        \"input_image\": wandb.Image(sample_image, caption=captions[0])\n",
    "    })\n",
    "\n",
    "    # Generate and log output image + clip score\n",
    "    gen_prompt = \"Á±≥Ëäæ Ë°å‰π¶\"\n",
    "    generate_and_log_image(pipe, clip_model, clip_processor, device, gen_prompt, epoch)\n",
    "\n",
    "\n",
    "save_lora_weights(pipe.unet, path=\"lora_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNvSdhEF2ke3"
   },
   "source": [
    "## Generate images with Trained LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLte_ZaOKxOW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Load LoRA weights and patch UNet\n",
    "state_dict = torch.load(\"lora_weights.pth\", map_location=device)\n",
    "patch_unet_with_lora(pipe.unet, r=lora_rank, alpha=lora_alpha)\n",
    "pipe.unet.load_state_dict(state_dict, strict=False)\n",
    "pipe.unet.eval()\n",
    "\n",
    "# Setup output\n",
    "output_dir = \"generated_images\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define prompts\n",
    "chinese_prompts = [\n",
    "    (\"Á±≥Ëäæ ‰π¶Ê≥ï\", \"Á±≥Ëäæ_‰π¶Ê≥ï_generated\"),\n",
    "    (\"Ë§öÈÅÇËâØ ‰π¶Ê≥ï\", \"Ë§öÈÅÇËâØ_‰π¶Ê≥ï_generated\"),\n",
    "    (\"‰π¶Ê≥ï\", \"‰π¶Ê≥ï_generated\")\n",
    "]\n",
    "\n",
    "# Define prompts\n",
    "english_prompts = [\n",
    "    (\"mi fu chinese caligraphy\", \"Á±≥Ëäæ_caligraphy_generated\"),\n",
    "    (\"chu suiliang chinese caligraphy\", \"Ë§öÈÅÇËâØ_caligraphy_generated\"),\n",
    "    (\"chinese caligraphy\", \"caligraphy_generated\")\n",
    "]\n",
    "\n",
    "baseline_prompts = [\n",
    "    # not in training set\n",
    "    (\"ÈÇìÁü≥Â¶Ç Èö∂‰π¶\", \"ÈÇìÁü≥Â¶Ç_Èö∂‰π¶_generated\"),\n",
    "    (\"deng shiru chinese caligraphy\", \"ÈÇìÁü≥Â¶Ç_caligraphy_generated\"),\n",
    "\n",
    "    # some english non caligraphy prompt\n",
    "    (\"a beautiful sunset in California\", \"sunset_generated\"),\n",
    "    (\"a cute cat\", \"cute_cat_generated\"),\n",
    "]\n",
    "\n",
    "all_propmpts = chinese_prompts + english_prompts + baseline_prompts\n",
    "\n",
    "def generate_log_display_images(prompt, filename_prefix):\n",
    "    for i in range(3):\n",
    "        image = pipe(prompt, num_inference_steps=30).images[0]\n",
    "        clip_score = calculate_clip_score(\n",
    "            image, prompt, clip_model, clip_processor, device\n",
    "        )\n",
    "\n",
    "        # wandb.log({\n",
    "        #     \"prompt\": prompt,\n",
    "        #     \"clip_score\": clip_score,\n",
    "        #     \"output image\": wandb.Image(image, caption=f\"{prompt} | score: {clip_score:.3f}\")\n",
    "        # })\n",
    "        print (f\"{prompt} | clip_score {clip_score}\")\n",
    "        image_path = os.path.join(output_dir, f\"{filename_prefix}_{i}.png\")\n",
    "        image.save(image_path)\n",
    "        image.show()\n",
    "        display(image)  # for inline notebook display\n",
    "\n",
    "# Run generation for all prompts\n",
    "for prompt_text, filename_prefix in all_propmpts:\n",
    "    generate_log_display_images(prompt_text, filename_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEht4aJWC9kF"
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
