{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP4vxrL4TUXQnWS8B/Fv1tT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8q4pyxU2Dzt3"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","import os\n","import sys\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = \"cs231n/lora_w_diffusion-main/\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","PROJECT_PATH = f\"/content/drive/My Drive/{FOLDERNAME}\"\n","sys.path.append(PROJECT_PATH)\n","\n","# Change working directory\n","os.chdir(PROJECT_PATH)\n","\n","# Confirm\n","print(\"‚úÖ Current working directory:\", os.getcwd())\n","print(\"üìÅ Contents:\", os.listdir('.'))"]},{"cell_type":"code","source":["!pip install -q diffusers transformers torchvision wandb torchmetrics[image]"],"metadata":{"id":"3-jK5YwgED7x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load stable diffusion model trained with LoRA"],"metadata":{"id":"DrgTMAfUK0Ci"}},{"cell_type":"code","source":["# Model params\n","lora_rank = 2\n","lora_alpha = 1\n","epochs = 1\n","lr = 1e-4\n","dropout = 0.1\n","dataset = \"data_chinese\"\n","agumentation = True\n","conv_lora = True\n","\n","device = \"cuda\"\n","\n","run_name = f\"dataset_{dataset}_rank_{lora_rank}_alpha_{lora_alpha}_epochs_{epochs}_conv_{int(conv_lora)}_lr_{lr:.0e}_dropout_{dropout}_augment_{int(agumentation)}\"\n","print(run_name)"],"metadata":{"id":"kIbu9HWBKweI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load base stable diffusion model"],"metadata":{"id":"ZeVA1CNdLHLR"}},{"cell_type":"code","source":["from lora import LoRALinear, LoRAConv2d\n","from patch_unet import patch_unet_with_lora, conv_filter\n","\n","from diffusers import StableDiffusionPipeline\n","import torch.nn as nn\n","import torch\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float32).to(device)"],"metadata":{"id":"hRjmsQCSLDJL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load LoRA weights and patch unet"],"metadata":{"id":"aiCbS2OlYpuM"}},{"cell_type":"code","source":["import torch\n","\n","model_path = f\"model_weights/{run_name}.pth\"\n","\n","pipe.vae.requires_grad_(False)\n","pipe.text_encoder.requires_grad_(False)\n","pipe.unet.requires_grad_(False)\n","\n","# ADD LORA\n","patch_unet_with_lora(pipe.unet, r=lora_rank, alpha=lora_alpha)\n","# patch_unet_with_lora(pipe.unet, r=lora_rank, alpha=lora_alpha, dropout=dropout, conv_filter=None)\n","pipe.unet.to(device)  # Move after patching\n","\n","state_dict = torch.load(model_path, map_location=device)\n","pipe.unet.load_state_dict(state_dict, strict=False)\n","pipe.unet.eval()"],"metadata":{"id":"zKP_Xsj2NcDG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Contrastive Language-Image Pre-training (CLIP)\n","\n","CLIP learns relationships between images and text. It encodes images and text and computes similarity base on semantics\n","\n","* Image Encoder: Processes images and converts them into a high-dimensional vector representation (embedding) that captures the visual features of the image.\n","* Text Encoder: Processes text descriptions and converts them into a high-dimensional vector representation (embedding) that captures the semantic meaning of the text.\n"],"metadata":{"id":"KU_r2kKcN3yq"}},{"cell_type":"code","source":["# Loads the CLIP model and processor\n","from transformers import CLIPProcessor, CLIPModel\n","\n","clip_model_name =\"openai/clip-vit-large-patch14\" # Consistent with CompVis/stable-diffusion-v1-4\n","clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n","clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n","\n","# Utility to calculates clip similarity\n","from clip import calculate_clip_score"],"metadata":{"id":"oxmi3bo6NfKo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate image folders from model"],"metadata":{"id":"MQAmvTJtEPuy"}},{"cell_type":"markdown","source":["### Set up parameters"],"metadata":{"id":"UlwjBGoqEhmp"}},{"cell_type":"code","source":["num_image_to_generate_per_prompt = 1\n","log_generated_image = True\n","\n","# Define prompts\n","author_chinese = [\"Á±≥Ëäæ\", \"Ë§öÈÅÇËâØ\", \"ÈÇìÁü≥Â¶Ç\", \"‰π¶Ê≥ïÂÆ∂\"]\n","author_english = [\"mi fu\", \"chu sui liang\", \"deng shi ru\", \"calligrapher\"]\n","script_chinese = [\"Â≠ó‰Ωì\", \"‰π¶Ê≥ï\", \"Ê•∑‰π¶\", \"Ë°å‰π¶\", \"Ëçâ‰π¶\", \"Èö∂‰π¶\"]\n","script_english = [\"script\", \"calligraphy\", \"regular script\", \"semi-cursive script\", \"cursive script\", \"clerical script\"]\n","unrelated = [\"cat\", \"tree\", \"dog\", \"sunrise\", \"sunset\"]\n","confusion = [\"email\", \"text mesage\", \"paper\"]\n","\n","# Setup generated image path\n","base_output_dir = f\"generated_images/{run_name}\"\n","os.makedirs(base_output_dir, exist_ok=True)"],"metadata":{"id":"8vt5pzM9EPIb","executionInfo":{"status":"ok","timestamp":1748813066368,"user_tz":420,"elapsed":8,"user":{"displayName":"Jieshu Huang (Jessica)","userId":"04738823243622637046"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["if log_generated_image:\n","  import wandb\n","  import os\n","  os.environ[\"WANDB_API_KEY\"] = \"d3cb18c48782c92319e4f2a53d26a05e702caa3e\"\n","  wandb.login()\n","\n","  wandb.init(\n","      project=\"stable-diffusion-calligraphy\",\n","      name=run_name,  # give each experiment a unique name\n","      config={\n","          \"lora_rank\": lora_rank,\n","          \"lora_alpha\": lora_alpha,\n","          \"lr\": lr,\n","          \"epochs\": epochs,\n","          \"augmentations\": False,\n","          \"conv_lora\": conv_lora,\n","      }\n","  )"],"metadata":{"id":"6oDgf-PXIcM1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate images by prompts"],"metadata":{"id":"bnCAPiyUH_bc"}},{"cell_type":"code","source":["def generate_image(prompt, output_dir, filename_prefix):\n","    \"\"\"\n","    Generates images based on a text prompt using a Stable Diffusion pipeline,\n","    calculates a CLIP score for each generated image, saves the image, and\n","    optionally logs the image and its score to Weights & Biases (wandb).\n","    \"\"\"\n","    for i in range(num_image_to_generate_per_prompt):\n","        image = pipe(prompt, num_inference_steps=30).images[0]\n","        clip_score = calculate_clip_score(\n","            image, prompt, clip_model, clip_processor, device\n","        )\n","        print (f\"{prompt} | clip_score {clip_score}\")\n","        image_path = os.path.join(output_dir, f\"{filename_prefix}_{i}.png\")\n","        image.save(image_path)\n","        if log_generated_image:\n","          wandb.log({\n","            \"prompt\": prompt,\n","            \"clip_score\": clip_score,\n","            \"output image\": wandb.Image(image, caption=f\"{prompt} | score: {clip_score:.3f}\")\n","          })\n","          image.show()\n","          display(image)  # for inline notebook display\n","\n","def generate_caligraphy_images(author_list, script_list, sub_dir):\n","    '''\n","    Generates calligraphy images for a list of authors and scripts, saving them\n","    into a structured directory\n","    '''\n","    output_dir = os.path.join(base_output_dir, sub_dir)\n","    for author in author_list:\n","      # Create a subdirectory for each author to compare style\n","      author_dir = os.path.join(output_dir, author.replace(\" \", \"_\"))\n","      os.makedirs(author_dir, exist_ok=True)\n","      for script in script_list:\n","          prompt = f\"{author} {script}\"\n","          filename_prefix = prompt.replace(\" \", \"_\")\n","          generate_image(prompt, author_dir, filename_prefix)"],"metadata":{"id":"roZifp0QGAMt","executionInfo":{"status":"ok","timestamp":1748813077451,"user_tz":420,"elapsed":5,"user":{"displayName":"Jieshu Huang (Jessica)","userId":"04738823243622637046"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Generate calligraphy with chinese prompt\n","generate_caligraphy_images(author_chinese, script_chinese, \"chinese\")\n","\n","# Generate calligraphy with english prompt\n","generate_caligraphy_images(author_english, script_english, \"english\")\n","\n","# Generate images with unrelated prompt\n","output_dir = os.path.join(base_output_dir, \"unrelated\")\n","os.makedirs(output_dir, exist_ok=True)\n","for prompt in unrelated:\n","    filename_prefix = prompt\n","    generate_image(prompt, output_dir, filename_prefix)\n","\n","# Generate images with confusion prompt\n","output_dir = os.path.join(base_output_dir, \"confusion\")\n","os.makedirs(output_dir, exist_ok=True)\n","for prompt in confusion:\n","    filename_prefix = prompt\n","    generate_image(prompt, output_dir, filename_prefix)"],"metadata":{"id":"w1p26_3KGEGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if log_generated_image:\n","  wandb.finish()"],"metadata":{"id":"7i2lu4kJG-ig"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[" # Frechet Inception Distance (FID)\n","\n"," FID assesses the similarity between a set of generated images and a set of real images by comparing their statistical properties in a feature space learned by the Inception v3 deep learning model.\n","\n"," A lower FID score indicates that the statistical distribution of the generated images is more similar to that of the real images. A score of 0 would imply that the two distributions are identical.\n","\n"," FID = $||\\mu_r - \\mu_g||^2 + Tr(\\Sigma_r + \\Sigma_g - 2\\sqrt{\\Sigma_r \\Sigma_g}$"],"metadata":{"id":"PJsIn1qvJPIQ"}},{"cell_type":"markdown","source":["### Set up parameters for preprocess images"],"metadata":{"id":"8zia2n23KFg9"}},{"cell_type":"code","source":["# Parameter for generate images and evaluation\n","from torchvision import transforms\n","from fid import load_images_from_folder, calculate_fid_score, add_fake_data_and_calculate_fid_score, calculate_fid_score_between_folders\n","\n","# TODO: Tune transform to capture calligraphy related features in FID\n","# Preprocess images:\n","target_size = (299, 299)\n","# Resize and transform to gray scale.\n","transform = transforms.Compose([\n","  transforms.Resize(target_size),\n","  transforms.Grayscale(num_output_channels=3), # Convert to grayscale with 1 output channel\n","])\n","\n","device = \"cuda\""],"metadata":{"id":"rEHyIiEMJOVs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Baseline: FID between two real images datasets"],"metadata":{"id":"cHjnC3mqJLm3"}},{"cell_type":"code","source":["import os\n","from torchmetrics.image.fid import FrechetInceptionDistance\n","# Baseline: fid similarity between real datasets\n","\n","base_input_dir = \"data_curation\"\n","show_example = False\n","\n","mifu = \"Á±≥Ëäæ\"\n","chu = \"Ë§öÈÅÇËâØ/Ë§öÈÅÇËâØ_ÂçÉÂ≠óÊñá\"\n","cat = \"cat\"\n","sunrise = \"sunrise\"\n","\n","mifu_real_image_path = os.path.join(base_input_dir, \"chinese\", mifu)\n","chu_real_image_path = os.path.join(base_input_dir, \"chinese\", chu)\n","cat_real_image_path = os.path.join(base_input_dir, \"unrelated\", cat)\n","sunrise_real_image_path = os.path.join(base_input_dir, \"unrelated\", sunrise)\n","\n","_, mifu_real_images = load_images_from_folder(mifu_real_image_path, transform, device, show_example)\n","_, chu_real_images = load_images_from_folder(chu_real_image_path, transform, device, show_example)\n","_, cat_real_images = load_images_from_folder(cat_real_image_path, transform, device, show_example)\n","_, sunrise_real_images = load_images_from_folder(sunrise_real_image_path, transform, device, show_example)\n","\n","# FID real mifu and real mifu\n","score = calculate_fid_score(mifu_real_images, mifu_real_images, device)\n","print (f\"fid score between {mifu} {mifu} : {score}\")\n","\n","# FID real mifu and real chu sui liang\n","score = calculate_fid_score(chu_real_images, mifu_real_images, device)\n","print (f\"fid score between {chu} {mifu}: {score}\")\n","\n","# FID real mifu and real cat\n","score = calculate_fid_score(cat_real_images, mifu_real_images, device)\n","print (f\"fid score between {cat} {mifu} : {score}\")\n","\n","# FID real mifu and real sun rise\n","score = calculate_fid_score(sunrise_real_images, mifu_real_images, device)\n","print (f\"fid score between {sunrise} {mifu} : {score}\")"],"metadata":{"id":"9S1D-cZlJLIH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluate generated images: FID between generated image and real images dataset"],"metadata":{"id":"vVbn5zJ8SVCS"}},{"cell_type":"code","source":["show_example = True\n","\n","all_generated_images = []\n","for author in author_chinese:\n","  generated_path = os.path.join(base_output_dir, \"chinese\", author.replace(\" \", \"_\"))\n","  _, generated_images = load_images_from_folder(generated_path, transform, device, show_example)\n","  all_generated_images.append((generated_images, author))\n","\n","for author in author_english:\n","  generated_path = os.path.join(base_output_dir, \"english\", author.replace(\" \", \"_\"))\n","  _, generated_images = load_images_from_folder(generated_path, transform, device, show_example)\n","  all_generated_images.append((generated_images, author))\n","\n","unrelated_path = os.path.join(base_output_dir, \"unrelated\")\n","_, generated_images = load_images_from_folder(unrelated_path, transform, device, show_example)\n","all_generated_images.append((generated_images, \"unrelated\"))\n","\n","confusion_path = os.path.join(base_output_dir, \"confusion\")\n","_, generated_images = load_images_from_folder(confusion_path, transform, device, show_example)\n","all_generated_images.append((generated_images, \"confusion\"))\n","\n","# FID with real mi fu\n","for imgs in all_generated_images:\n","  images, image_name = imgs\n","  score = calculate_fid_score(images, mifu_real_images, device)\n","  print (f\"fid score between [{image_name}] and [real mi fu]: {score}\")\n","\n","# FID with real chu sui liang\n","for imgs in all_generated_images:\n","  images, image_name = imgs\n","  score = calculate_fid_score(images, chu_real_images, device)\n","  print (f\"fid score between [{image_name}] and [real chu sui liang]: {score}\")\n","\n","# FID with real cat\n","for imgs in all_generated_images:\n","  images, image_name = imgs\n","  score = calculate_fid_score(images, cat_real_images, device)\n","  print (f\"fid score between [{image_name}] and [real cat]: {score}\")\n","\n","# FID with real sun rise\n","for imgs in all_generated_images:\n","  images, image_name = imgs\n","  score = calculate_fid_score(images, sunrise_real_images, device)\n","  print (f\"fid score between [{image_name}] and [real sunrise]: {score}\")"],"metadata":{"id":"QiWALsFRSe-J"},"execution_count":null,"outputs":[]}]}