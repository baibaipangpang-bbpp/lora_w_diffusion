{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtM0VRMI3Nb1"
      },
      "outputs": [],
      "source": [
        "# This mounts your Google Drive to the Colab VM.\n",
        "from google.colab import drive\n",
        "import os\n",
        "import sys\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
        "# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n",
        "FOLDERNAME = \"cs231n/project/\"\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "PROJECT_PATH = f\"/content/drive/My Drive/{FOLDERNAME}\"\n",
        "sys.path.append(PROJECT_PATH)\n",
        "\n",
        "# Change working directory\n",
        "os.chdir(PROJECT_PATH)\n",
        "\n",
        "# Confirm\n",
        "print(\"‚úÖ Current working directory:\", os.getcwd())\n",
        "print(\"üìÅ Contents:\", os.listdir('.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRetkHCR2ke0"
      },
      "source": [
        "# üí° LoRA Training on Stable Diffusion\n",
        "This notebook trains custom LoRA adapters on Stable Diffusion using your own image-caption pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnov-IWl2ke1"
      },
      "outputs": [],
      "source": [
        "!pip install -q diffusers transformers accelerate torchvision safetensors kornia wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjd5UBjt2ke3"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4907-Xf2ke3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from dataset import ImageTextDataset\n",
        "\n",
        "dataset = ImageTextDataset(\"data\")\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# sanity check for datasets\n",
        "print(f\"Number of image-caption pairs: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgZYv9Qr-fuV"
      },
      "outputs": [],
      "source": [
        "# preview a couple of images.\n",
        "from matplotlib import pyplot as plt\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "# This regex will match messages containing \"Glyph\" and \"missing from font\"\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\"Glyph .* missing from font\\(s\\) DejaVu Sans\\.\")\n",
        "\n",
        "for i in range(min(2, len(dataset))):  # Show up to 3 examples\n",
        "    image, caption = dataset[i]\n",
        "    plt.imshow(image.permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Undo normalization\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(caption)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejszCma45YBl"
      },
      "source": [
        "## Training Logic for Lora Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWHHLkRet1N_"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"] = \"20e7c4be307028d246fbff111508b75d9eaab1ee\"\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(\n",
        "    project=\"stable-diffusion-calligraphy\",\n",
        "    name=\"Á±≥Ëäæ_Ë§öÈÅÇËâØ1_10epock_agumentNone_EnglishData\",  # give each experiment a unique name\n",
        "    config={\n",
        "        \"lora_rank\": 8,\n",
        "        \"lora_alpha\": 8,\n",
        "        \"lr\": 1e-4,\n",
        "        \"epochs\": 2,\n",
        "        \"augmentations\": False,\n",
        "        \"conv_lora\": True,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LsuFYFH783B"
      },
      "outputs": [],
      "source": [
        "from lora import LoRALinear, LoRAConv2d\n",
        "from patch_unet import patch_unet_with_lora, conv_filter\n",
        "\n",
        "device = \"cuda\"\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float32).to(device)\n",
        "\n",
        "pipe.vae.requires_grad_(False)\n",
        "pipe.text_encoder.requires_grad_(False)\n",
        "pipe.unet.requires_grad_(False)\n",
        "\n",
        "# ADD LORA\n",
        "patch_unet_with_lora(pipe.unet, r=8, alpha=8, dropout=0.0, conv_filter=None)\n",
        "\n",
        "pipe.unet.to(device)  # Move after patching\n",
        "\n",
        "# unfreeze lora weights\n",
        "for module in pipe.unet.modules():\n",
        "    if isinstance(module, (LoRALinear, LoRAConv2d)):\n",
        "        for p in module.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam([p for p in pipe.unet.parameters() if p.requires_grad], lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oQFjHrnQ_hS"
      },
      "outputs": [],
      "source": [
        "# patch_unet_with_lora(\n",
        "#     pipe.unet,\n",
        "#     r=4,\n",
        "#     alpha=1.0,\n",
        "#     dropout=0.1,\n",
        "#     enable_linear=False,\n",
        "#     enable_conv=True,\n",
        "#     conv_filter=lambda name, mod: \"down_blocks\" in name and mod.kernel_size == (3, 3)\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loads the CLIP model and processor\n",
        "from clip import calculate_clip_score\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)"
      ],
      "metadata": {
        "id": "hJ9tAhsokBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_log_image(pipe, clip_model, clip_processor, device, prompt, epoch):\n",
        "    \"\"\"\n",
        "    Generate an image from a prompt using the pipeline, calculate CLIP score,\n",
        "    and log the result to Weights & Biases.\n",
        "\n",
        "    Args:\n",
        "        pipe: The diffusion pipeline.\n",
        "        clip_model: The CLIP model for scoring.\n",
        "        clip_processor: The processor for CLIP input.\n",
        "        device: torch.device to use.\n",
        "        prompt (str): Text prompt to generate image from.\n",
        "        epoch (int): Current training epoch, for logging.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        image = pipe(prompt, num_inference_steps=30).images[0]\n",
        "\n",
        "    # Calculate CLIP score\n",
        "    clip_score = calculate_clip_score(image, prompt, clip_model, clip_processor, device)\n",
        "\n",
        "    # Log image, score, and prompt to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"generated_image\": wandb.Image(image, caption=f\"{prompt} | score: {clip_score:.3f}\"),\n",
        "        \"clip_score\": clip_score,\n",
        "        \"generation_prompt\": prompt,\n",
        "    })\n",
        "\n",
        "    return image, clip_score  # optionally return for other use"
      ],
      "metadata": {
        "id": "938GzVt2lwta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52d-NbgAA3nY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from augment import augment\n",
        "from patch_unet import save_lora_weights, load_lora_weights\n",
        "import torch.nn as nn\n",
        "\n",
        "# training\n",
        "for epoch in range(3):\n",
        "    for i, (images, captions) in enumerate(loader):\n",
        "        # Keep images in float32\n",
        "        images = images.to(device)\n",
        "\n",
        "        images = augment(images)\n",
        "\n",
        "        text_input = pipe.tokenizer(captions, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n",
        "        text_embeds = pipe.text_encoder(**text_input).last_hidden_state.to(device)  # stays float32\n",
        "\n",
        "        with torch.no_grad():\n",
        "            vae_output = pipe.vae.encode(images)\n",
        "        latents = vae_output.latent_dist.sample().to(device) * 0.18215  # float32\n",
        "\n",
        "        noise = torch.randn_like(latents).to(device)  # float32 noise\n",
        "        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n",
        "\n",
        "        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n",
        "\n",
        "        noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=text_embeds).sample  # float32\n",
        "\n",
        "        loss = nn.MSELoss()(noise_pred, noise)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch} Step {i} Loss: {loss.item():.4f}\")\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"step\": i,\n",
        "            \"loss\": loss.item(),\n",
        "            \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "        })\n",
        "\n",
        "    # Log one of the input images\n",
        "    sample_image = (images[0].cpu().numpy() * 0.5 + 0.5).transpose(1, 2, 0)\n",
        "    wandb.log({\n",
        "        \"input_image\": wandb.Image(sample_image, caption=captions[0])\n",
        "    })\n",
        "\n",
        "    # Generate and log output image + clip score\n",
        "    gen_prompt = \"chinese calligraphy\"\n",
        "    generate_and_log_image(pipe, clip_model, clip_processor, device, gen_prompt, epoch)\n",
        "\n",
        "\n",
        "save_lora_weights(pipe.unet, path=\"lora_weights.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNvSdhEF2ke3"
      },
      "source": [
        "## Generate images with Trained LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLte_ZaOKxOW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "# Load LoRA weights and patch UNet\n",
        "state_dict = torch.load(\"lora_weights.pth\", map_location=device)\n",
        "patch_unet_with_lora(pipe.unet, r=4, alpha=1.0)\n",
        "pipe.unet.load_state_dict(state_dict, strict=False)\n",
        "pipe.unet.eval()\n",
        "\n",
        "# Setup output\n",
        "output_dir = \"generated_images\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define prompts\n",
        "chinese_prompts = [\n",
        "    (\"Á±≥Ëäæ ‰π¶Ê≥ï\", \"Á±≥Ëäæ_‰π¶Ê≥ï_generated\"),\n",
        "    (\"Ë§öÈÅÇËâØ ‰π¶Ê≥ï\", \"Ë§öÈÅÇËâØ_‰π¶Ê≥ï_generated\"),\n",
        "    (\"‰π¶Ê≥ï\", \"‰π¶Ê≥ï_generated\")\n",
        "]\n",
        "\n",
        "# Define prompts\n",
        "english_prompts = [\n",
        "    (\"mi fu chinese caligraphy\", \"Á±≥Ëäæ_caligraphy_generated\"),\n",
        "    (\"chu suiliang chinese caligraphy\", \"Ë§öÈÅÇËâØ_caligraphy_generated\"),\n",
        "    (\"deng shiru chinese caligraphy\", \"ÈÇìÁü≥Â¶Ç_caligraphy_generated\"),\n",
        "    (\"chinese caligraphy\", \"caligraphy_generated\")\n",
        "]\n",
        "\n",
        "\n",
        "def generate_and_log_images(prompt, filename_prefix):\n",
        "    for i in range(3):\n",
        "        image = pipe(prompt, num_inference_steps=30).images[0]\n",
        "        clip_score = calculate_clip_score(\n",
        "            image, prompt, clip_model, clip_processor, device\n",
        "        )\n",
        "\n",
        "        wandb.log({\n",
        "            \"prompt\": prompt,\n",
        "            \"clip_score\": clip_score,\n",
        "            \"output image\": wandb.Image(image, caption=f\"{prompt} | score: {clip_score:.3f}\")\n",
        "        })\n",
        "\n",
        "        image_path = os.path.join(output_dir, f\"{filename_prefix}_{i}.png\")\n",
        "        image.save(image_path)\n",
        "        image.show()\n",
        "        display(image)  # for inline notebook display\n",
        "\n",
        "# Run generation for all prompts\n",
        "for prompt_text, filename_prefix in english_prompts:\n",
        "    generate_and_log_images(prompt_text, filename_prefix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEht4aJWC9kF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d380c704-c87f-48a5-b753-3e447962acac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "updating run config (3.7m)<br>  <strong style=\"color:red\">ERROR</strong> retrying HTTP 409: run odmeo5m6 was previously created and deleted; try a new run name"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}