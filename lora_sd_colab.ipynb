{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mtM0VRMI3Nb1"},"outputs":[],"source":["# This mounts your Google Drive to the Colab VM.\n","from google.colab import drive\n","import os\n","import sys\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","# TODO: Enter the foldername in your Drive where you have saved the unzipped\n","# assignment folder, e.g. 'cs231n/assignments/assignment3/'\n","FOLDERNAME = \"cs231n/lora_w_diffusion-main/\"\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","PROJECT_PATH = f\"/content/drive/My Drive/{FOLDERNAME}\"\n","sys.path.append(PROJECT_PATH)\n","\n","# Change working directory\n","os.chdir(PROJECT_PATH)\n","\n","# Confirm\n","print(\"‚úÖ Current working directory:\", os.getcwd())\n","print(\"üìÅ Contents:\", os.listdir('.'))"]},{"cell_type":"markdown","metadata":{"id":"TRetkHCR2ke0"},"source":["# üí° LoRA Training on Stable Diffusion\n","This notebook trains custom LoRA adapters on Stable Diffusion using your own image-caption pairs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wnov-IWl2ke1"},"outputs":[],"source":["!pip install -q diffusers transformers accelerate torchvision safetensors kornia wandb torchmetrics[image]"]},{"cell_type":"markdown","metadata":{"id":"Tjd5UBjt2ke3"},"source":["## Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcqYvlZ_1APe"},"outputs":[],"source":["# Hyper params\n","lora_rank = 2\n","lora_alpha = 1\n","epochs = 1\n","lr = 1e-4\n","dropout = 0.1\n","dataset = \"data_chinese\"\n","in_training_prompt = \"Á±≥Ëäæ Ë°å‰π¶\"\n","num_img_to_generate = 3\n","agumentation = True\n","conv_lora = True\n","device = \"cuda\"\n","\n","run_name = f\"dataset_{dataset}_rank_{lora_rank}_alpha_{lora_alpha}_epochs_{epochs}_conv_{int(conv_lora)}_lr_{lr:.0e}_dropout_{dropout}_augment_{int(agumentation)}\"\n","print(run_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4907-Xf2ke3"},"outputs":[],"source":["import os\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from dataset import ImageTextDataset\n","\n","dataset = ImageTextDataset(dataset)\n","loader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# sanity check for datasets\n","print(f\"Number of image-caption pairs: {len(dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgZYv9Qr-fuV"},"outputs":[],"source":["# preview a couple of images.\n","from matplotlib import pyplot as plt\n","import warnings\n","import re\n","\n","# This regex will match messages containing \"Glyph\" and \"missing from font\"\n","warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\"Glyph .* missing from font\\(s\\) DejaVu Sans\\.\")\n","\n","for i in range(min(2, len(dataset))):  # Show up to 3 examples\n","    image, caption = dataset[i]\n","    plt.imshow(image.permute(1, 2, 0).numpy() * 0.5 + 0.5)  # Undo normalization\n","    plt.axis(\"off\")\n","    plt.title(caption)\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ejszCma45YBl"},"source":["## Training Logic for Lora Finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWHHLkRet1N_"},"outputs":[],"source":["import wandb\n","import os\n","os.environ[\"WANDB_API_KEY\"] = \"20e7c4be307028d246fbff111508b75d9eaab1ee\"\n","wandb.login()\n","\n","wandb.init(\n","    project=\"stable-diffusion-calligraphy\",\n","    name=run_name,  # give each experiment a unique name\n","    config={\n","        \"lora_rank\": lora_rank,\n","        \"lora_alpha\": lora_alpha,\n","        \"lr\": lr,\n","        \"epochs\": epochs,\n","        \"augmentations\": False,\n","        \"conv_lora\": conv_lora,\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LsuFYFH783B"},"outputs":[],"source":["from lora import LoRALinear, LoRAConv2d\n","from patch_unet import patch_unet_with_lora, conv_filter\n","\n","from diffusers import StableDiffusionPipeline\n","import torch.nn as nn\n","\n","\n","pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float32).to(device)\n","\n","pipe.vae.requires_grad_(False)\n","pipe.text_encoder.requires_grad_(False)\n","pipe.unet.requires_grad_(False)\n","\n","# ADD LORA\n","patch_unet_with_lora(pipe.unet, r=lora_rank, alpha=lora_alpha, dropout=dropout, conv_filter=None)\n","\n","pipe.unet.to(device)  # Move after patching\n","\n","# unfreeze lora weights\n","for module in pipe.unet.modules():\n","    if isinstance(module, (LoRALinear, LoRAConv2d)):\n","        for p in module.parameters():\n","            p.requires_grad = True\n","\n","optimizer = torch.optim.Adam([p for p in pipe.unet.parameters() if p.requires_grad], lr=lr)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJ9tAhsokBbH"},"outputs":[],"source":["# Loads the CLIP model and processor\n","from clip import calculate_clip_score\n","from transformers import CLIPProcessor, CLIPModel\n","\n","clip_model_name =\"openai/clip-vit-large-patch14\" # Consistent with CompVis/stable-diffusion-v1-4\n","clip_model = CLIPModel.from_pretrained(clip_model_name).to(device)\n","clip_processor = CLIPProcessor.from_pretrained(clip_model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"938GzVt2lwta"},"outputs":[],"source":["def generate_and_log_image(pipe, clip_model, clip_processor, device, prompt, epoch):\n","    \"\"\"\n","    Generate an image from a prompt using the pipeline, calculate CLIP score,\n","    and log the result to Weights & Biases.\n","\n","    Args:\n","        pipe: The diffusion pipeline.\n","        clip_model: The CLIP model for scoring.\n","        clip_processor: The processor for CLIP input.\n","        device: torch.device to use.\n","        prompt (str): Text prompt to generate image from.\n","        epoch (int): Current training epoch, for logging.\n","    \"\"\"\n","    with torch.no_grad():\n","        image = pipe(prompt, num_inference_steps=30).images[0]\n","\n","    # Calculate CLIP score\n","    clip_score = calculate_clip_score(image, prompt, clip_model, clip_processor, device)\n","\n","    # Log image, score, and prompt to wandb\n","    wandb.log({\n","        \"epoch\": epoch,\n","        \"generated_image\": wandb.Image(image, caption=f\"{prompt} | score: {clip_score:.3f}\"),\n","        \"clip_score\": clip_score,\n","    })\n","\n","    return image, clip_score  # optionally return for other use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"52d-NbgAA3nY"},"outputs":[],"source":["import torch\n","from augment import augment\n","from patch_unet import save_lora_weights, load_lora_weights\n","import torch.nn as nn\n","\n","# Save model run with different hyper parameter separately\n","model_path = f\"model_weights/{run_name}.pth\"\n","\n","# training\n","for epoch in range(epochs):\n","    for i, (images, captions) in enumerate(loader):\n","        # Keep images in float32\n","        images = images.to(device)\n","\n","        images = augment(images)\n","\n","        text_input = pipe.tokenizer(captions, padding=\"max_length\", max_length=77, return_tensors=\"pt\").to(device)\n","        text_embeds = pipe.text_encoder(**text_input).last_hidden_state.to(device)  # stays float32\n","\n","        with torch.no_grad():\n","            vae_output = pipe.vae.encode(images)\n","        latents = vae_output.latent_dist.sample().to(device) * 0.18215  # float32\n","\n","        noise = torch.randn_like(latents).to(device)  # float32 noise\n","        timesteps = torch.randint(0, 1000, (latents.shape[0],), device=device).long()\n","\n","        noisy_latents = pipe.scheduler.add_noise(latents, noise, timesteps)\n","\n","        noise_pred = pipe.unet(noisy_latents, timesteps, encoder_hidden_states=text_embeds).sample  # float32\n","\n","        loss = nn.MSELoss()(noise_pred, noise)\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        print(f\"Epoch {epoch} Step {i} Loss: {loss.item():.4f}\")\n","        wandb.log({\n","            \"epoch\": epoch,\n","            \"step\": i,\n","            \"loss\": loss.item(),\n","            \"lr\": optimizer.param_groups[0][\"lr\"],\n","        })\n","\n","    # Log one of the input images\n","    sample_image = (images[0].cpu().numpy() * 0.5 + 0.5).transpose(1, 2, 0)\n","    wandb.log({\n","        \"input_image\": wandb.Image(sample_image, caption=captions[0])\n","    })\n","\n","    # Generate and log output image + clip score\n","    generate_and_log_image(pipe, clip_model, clip_processor, device, in_training_prompt, epoch)\n","\n","\n","save_lora_weights(pipe.unet, path=model_path)"]},{"cell_type":"markdown","metadata":{"id":"lNvSdhEF2ke3"},"source":["## Generate images with Trained LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jLte_ZaOKxOW"},"outputs":[],"source":["import os\n","import torch\n","import wandb\n","\n","# Load LoRA weights and patch UNet\n","# model_path = \"lora_weights.pth\"\n","model_path = f\"model_weights/{run_name}.pth\"\n","\n","state_dict = torch.load(model_path, map_location=device)\n","patch_unet_with_lora(pipe.unet, r=lora_rank, alpha=lora_alpha)\n","pipe.unet.load_state_dict(state_dict, strict=False)\n","pipe.unet.eval()\n","\n","# Setup output\n","output_dir = \"generated_images\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Define prompts\n","chinese_prompts = [\n","    (\"Á±≥Ëäæ Ë°å‰π¶\", \"Á±≥Ëäæ_Ë°å‰π¶_generated\"),\n","    (\"Ë§öÈÅÇËâØ Ê•∑‰π¶\", \"Ë§öÈÅÇËâØ_Ê•∑‰π¶_generated\"),\n","    (\"Ê•∑‰π¶\", \"Ê•∑‰π¶_generated\")\n","]\n","\n","# Define prompts\n","english_prompts = [\n","    (\"mi fu chinese caligraphy xingshu\", \"Á±≥Ëäæ_caligraphy_generated\"),\n","    (\"chu suiliang chinese caligraphy kaishu\", \"Ë§öÈÅÇËâØ_caligraphy_generated\"),\n","    (\"chinese caligraphy kaishu\", \"kaishu_caligraphy_generated\")\n","]\n","\n","baseline_prompts = [\n","    # not in training set\n","    (\"ÈÇìÁü≥Â¶Ç Èö∂‰π¶\", \"ÈÇìÁü≥Â¶Ç_Èö∂‰π¶_generated\"),\n","    (\"deng shiru chinese caligraphy lishu\", \"ÈÇìÁü≥Â¶Ç_lishu_caligraphy_generated\"),\n","\n","    # some english non caligraphy prompt\n","    (\"a beautiful sunset in California\", \"sunset_generated\"),\n","    (\"a cute cat\", \"cute_cat_generated\"),\n","]\n","\n","all_propmpts = chinese_prompts + english_prompts + baseline_prompts\n","\n","def generate_log_display_images(prompt, filename_prefix):\n","    for i in range(num_img_to_generate):\n","        image = pipe(prompt, num_inference_steps=30).images[0]\n","        clip_score = calculate_clip_score(\n","            image, prompt, clip_model, clip_processor, device\n","        )\n","\n","        wandb.log({\n","            \"prompt\": prompt,\n","            \"clip_score\": clip_score,\n","            \"output image\": wandb.Image(image, caption=f\"{prompt} | score: {clip_score:.3f}\")\n","        })\n","        print (f\"{prompt} | clip_score {clip_score}\")\n","        image_path = os.path.join(output_dir, f\"{filename_prefix}_{i}.png\")\n","        image.save(image_path)\n","        image.show()\n","        display(image)  # for inline notebook display\n","\n","# Run generation for all prompts\n","for prompt_text, filename_prefix in all_propmpts:\n","    generate_log_display_images(prompt_text, filename_prefix)\n"]},{"cell_type":"markdown","metadata":{"id":"do-A_t_gjghL"},"source":["##[Optional] Visualize Layers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UW-R2I6Fje9H"},"outputs":[],"source":["import torch.nn as nn\n","\n","print(\"Available Conv2d layers in the UNet:\")\n","for name, module in pipe.unet.named_modules():\n","    if isinstance(module, nn.Conv2d):\n","        print(f\"- {name} (Output shape usually depends on input size)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"goOKZYccje2k"},"outputs":[],"source":["import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from patch_unet import remove_all_hooks, register_hooks, get_activation\n","\n","activations = {}\n","# --- Choose Layers and Register Hooks ---\n","layers_to_visualize = [\n","    'down_blocks.0.resnets.0.conv1',\n","    'up_blocks.0.resnets.0.conv1',\n","    'mid_block.resnets.0.conv2'\n","]\n","\n","# Clear previous hooks/activations if running multiple times\n","remove_all_hooks()\n","activations.clear()\n","\n","register_hooks(pipe.unet, layers_to_visualize)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oxr3BQXAjev9"},"outputs":[],"source":["prompt = \"Á±≥Ëäæ Ë°å‰π¶\"\n","generator = torch.Generator(device=device).manual_seed(42) # Use a seed for reproducibility\n","\n","print(\"Running inference...\")\n","image = pipe(prompt, num_inference_steps=30, generator=generator).images[0]\n","print(\"Inference complete.\")\n","\n","# It's crucial to remove hooks *after* inference\n","remove_all_hooks()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Liwbw1qnjecq"},"outputs":[],"source":["from patch_unet import plot_activations\n","\n","for layer_name in layers_to_visualize:\n","    # Choose batch_index=0 or batch_index=1 to see the difference\n","    plot_activations(layer_name, num_cols=16, scale=1.5, batch_index=0)\n","    #plot_activations(layer_name, num_cols=16, scale=1.5, batch_index=1)"]},{"cell_type":"markdown","metadata":{"id":"X1pBZJyf5KRo"},"source":["##Merge Lora weights back to the base stable diffusion model"]},{"cell_type":"code","source":["wandb.finish()"],"metadata":{"id":"9yPxHNUg95j6"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}